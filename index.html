<p><strong>HPCA 2019</strong></p>
<ol>
<li>HyPar: Torwards Hybrid Parallelism for Deep Learning Accelerator Array</li>
<li>E-RNN: Design Optimization for Efficient Recurrent Neural Networks in FPGAs</li>
<li>Bit Prudent In-Cache Acceleration of Deep Convolutional Neural Networks</li>
<li>Shortcut Mining: Exploiting Cross-Layer Shortcut Reuse in DCNN Accelerators</li>
<li>Reliability Evaluation of Mixed-Precision Architectures</li>
<li>Architecting Waferscale Processors - A GPU Case Study</li>
<li>Machine Learning at Facebook: Understanding Inference at the Edge</li>
<li>VIP: A Versatile Inference Processor</li>
<li>String Figure: A Scalable and Elastic Memory Network Architecture</li>
<li>NAND-Net: Minimizing Computational Complexity of In-Memory Processing of Binary Neural Network</li>
</ol>
<hr>
<p><strong>ASPLOS 2019</strong></p>
<ol>
<li>Intelligence Beyond the Edge: Inference on Intermittent Embedded Systems</li>
<li>Boosted Race Trees for Low Energy Classification</li>
<li>FA3C: FPGA-Accelerated Deep Reinforcement Learning</li>
<li>PUMA: A Programmable Ultra-Efficient Memristor-based Accelerator for Machine Learning Inference</li>
<li>FPSA: A Full System Stack Solution for Reconfigurable ReRAM-based NN Accelerator Architecture</li>
<li>Bit-Tactical A Software/Hardware Approach to Exploiting Value and Bit Sparsity in Neural Networks</li>
<li>TANGRAM: Optimized Coarse-Grained Dataflow for Scalable NN Accelerators</li>
<li>Packing Sparse CNNs for Efficient Systolic Array Implementations: Column Combining Under Joing Optimization</li>
<li>Split-CNN: Splitting Window-based Operations in CNNs for Memory System Optimization</li>
<li>HoP: Heterogeneity-Aware Decentralized Learning</li>
<li>Astra: Exploiting Predictability to Optimize Deep Learning</li>
<li>ADMM-NN: An Algorithm-Hardware Co-Design Framework of DNNs Using Alternating Direction Methods of Multipliers</li>
</ol>
<hr>
<p><strong>ISCA 2019</strong></p>
<ol>
<li>3D-based Video Understanding Accelerator by Leveraging Temporal Locality and Activation Sparsity</li>
<li>Sparse ReRAM Engine: Joint Exploration of Activatoin and Weight Sparsity on Compressed Neural Network</li>
<li>MnnFast: A Fast and Scalable System Architecture for Memory-Augmented Neural Networks</li>
<li>TIE: Energy-Efficient Tensor Train-Based Inference Engine for Deep Neural Network</li>
<li>Accelerating Distributed Reinforcement Learning with In-Switch Computing</li>
<li>Eager Pruning: Algorithm and Architecture Support for Fast Training of Deep Neural Networks</li>
<li>Laconic Deep Learning Inference Acceleration</li>
<li>Fractal Machine Learning Computers</li>
<li>FloatPIM: In-Memory Accelerator of Deep Neural Network Training with High Precision</li>
</ol>
<hr>
<p><strong>MICRO 2019</strong></p>
<ol>
<li>Wire-Aware Architecture and Dataflow for CNN Accelerators</li>
<li>Simba: Scaling Deep Learning Inference with Multi-Chip-Module based Accelerators</li>
<li>ShapeShifter: Enabling Fine-Grain data Width Adaption in Deep Learning</li>
<li>Connecting RRAMs to Extend Analog Dataflow in an End-to-End In-Memory Processing Paradigm</li>
<li>ZCOMP: Reducing DNN Cross-Layer Memory Footprint Using Vector Extensions</li>
