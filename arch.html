
<!DOCTYPE html>
<html>
<head>
<meta name="viewport" content="width=device-width, initial-scale=1">
<style>
body, html {
  height: 100%;
  margin: 0;
  font-family: Palatino;
}

.hero-image {
  background-image: linear-gradient(rgba(0, 0, 0, 0.5), rgba(0, 0, 0, 0.5)), url("papers_header.jpg");
  height: 50%;
  background-position: center;
  background-repeat: no-repeat;
  background-size: cover;
  position: relative;
}

.hero-text {
  text-align: center;
  position: absolute;
  top: 50%;
  left: 50%;
  transform: translate(-50%, -50%);
  color:crimson;
}
</style>
</head>
<body>

<div class="hero-image">
  <div class="hero-text">
    <h1 style="font-size:50px">Research paper list</h1>
    <p>A list of papers that align with my research interests</p>
  </div>
</div>


<br>
<strong>HPCA 2019</strong>
<ol>
<li>HyPar: Torwards Hybrid Parallelism for Deep Learning Accelerator Array</li>
<li>E-RNN: Design Optimization for Efficient Recurrent Neural Networks in FPGAs</li>
<li>Bit Prudent In-Cache Acceleration of Deep Convolutional Neural Networks</li>
<li>Shortcut Mining: Exploiting Cross-Layer Shortcut Reuse in DCNN Accelerators</li>
<li>Reliability Evaluation of Mixed-Precision Architectures</li>
<li>Architecting Waferscale Processors - A GPU Case Study</li>
<li>Machine Learning at Facebook: Understanding Inference at the Edge</li>
<li>VIP: A Versatile Inference Processor</li>
<li>String Figure: A Scalable and Elastic Memory Network Architecture</li>
<li>NAND-Net: Minimizing Computational Complexity of In-Memory Processing of Binary Neural Network</li>
</ol>
<hr>
<strong>ASPLOS 2019</strong>
<ol>
<li>Intelligence Beyond the Edge: Inference on Intermittent Embedded Systems</li>
<li>Boosted Race Trees for Low Energy Classification</li>
<li>FA3C: FPGA-Accelerated Deep Reinforcement Learning</li>
<li>PUMA: A Programmable Ultra-Efficient Memristor-based Accelerator for Machine Learning Inference</li>
<li>FPSA: A Full System Stack Solution for Reconfigurable ReRAM-based NN Accelerator Architecture</li>
<li>Bit-Tactical A Software/Hardware Approach to Exploiting Value and Bit Sparsity in Neural Networks</li>
<li>TANGRAM: Optimized Coarse-Grained Dataflow for Scalable NN Accelerators</li>
<li>Packing Sparse CNNs for Efficient Systolic Array Implementations: Column Combining Under Joing Optimization</li>
<li>Split-CNN: Splitting Window-based Operations in CNNs for Memory System Optimization</li>
<li>HoP: Heterogeneity-Aware Decentralized Learning</li>
<li>Astra: Exploiting Predictability to Optimize Deep Learning</li>
<li>ADMM-NN: An Algorithm-Hardware Co-Design Framework of DNNs Using Alternating Direction Methods of Multipliers</li>
</ol>
<hr>
<strong>ISCA 2019</strong>
<ol>
<li>3D-based Video Understanding Accelerator by Leveraging Temporal Locality and Activation Sparsity</li>
<li>Sparse ReRAM Engine: Joint Exploration of Activatoin and Weight Sparsity on Compressed Neural Network</li>
<li>MnnFast: A Fast and Scalable System Architecture for Memory-Augmented Neural Networks</li>
<li>TIE: Energy-Efficient Tensor Train-Based Inference Engine for Deep Neural Network</li>
<li>Accelerating Distributed Reinforcement Learning with In-Switch Computing</li>
<li>Eager Pruning: Algorithm and Architecture Support for Fast Training of Deep Neural Networks</li>
<li>Laconic Deep Learning Inference Acceleration</li>
<li>Fractal Machine Learning Computers</li>
<li>FloatPIM: In-Memory Accelerator of Deep Neural Network Training with High Precision</li>
</ol>
<hr>
<strong>MICRO 2019</strong>
<ol>
<li>Wire-Aware Architecture and Dataflow for CNN Accelerators</li>
<li>Simba: Scaling Deep Learning Inference with Multi-Chip-Module based Accelerators</li>
<li>ShapeShifter: Enabling Fine-Grain data Width Adaption in Deep Learning</li>
<li>Connecting RRAMs to Extend Analog Dataflow in an End-to-End In-Memory Processing Paradigm</li>
<li>ZCOMP: Reducing DNN Cross-Layer Memory Footprint Using Vector Extensions</li>
<li>Boosting the Performance of CNN Accelerators with Dynamic Fine Grained Channel Gating</li>
<li>SparTen: A Sparse Tensor Accelerator for Convolutional Neural Networks</li>
<li>EDEN: Enabling Energy-Efficient High-Performance Deep Neural Network Inference Using Approximate DRAM</li>
<li>eCNN: A Block-Based and Highly-Parallel CNN Accelerator for Edge Inference</li>
<li>FlexLearn: Fast and Highly Efficient Brain Simulations Using Flexible On-Chip Learning</li>
<li>ExTensor: An Accelerator for Sparse Tensor Algebra</li>
<li>Efficient SpMV Operation for Large and Highly Sparse Matrices Using Scalable Multi-way Merge Parallelization</li>
<li>Sparse Tensor Core: Algorithm and Hardware Co-Design for Vector-wise Sparse Neural Networks on Modern GPUs</li>
<li>SMASH: Co-designing Software Compression and Hardware-Accelerated Indexing for Efficient Sparse Matrix Operations</li>
<li>Tigris: Architecture and Algorithms for 3D Perception in Point Couds</li>
<li>ASV: Accelerated Stereo Vision System</li>
<li>Distilling the Essence of Raw Video to Reduce Memory Usage and Energy at Edge Devices</li>
<li>MANIC: A Vector-Dataflow Architecture for Ultra-Low-Power Embedded Systems</li>
<li>NetDIMM: Low-Latency Near-Memory Network Interface Architecture</li>
<li>TensorDIMM: A Practical Near-Memory Processing Architecture for Embeddings and Tensor Operations in Deep Learning </li>
<li>Understanding Reuse, Performance, and Hardware Cost of DNN Dataflow: A Data-Centric Approach</li>
<li>MaxNVM: Maximizing DNN Storage Density and Inference Efficiency with Sparse Encoding and Error Mitigation</li>
<li>Neuron-Level Fuzzy Memorization in RNNs</li>
<li>Manna: An Accelerator for Memory-Augmented Neural Networks</li>
</ol>
<hr>
<strong>HPCA 2020</strong>
<ol>
<li>SIGMA: A Sparse and Irregular GEMM Accelerator with Flexible Interconnects for DNN Training</li>
<li>A^3: Accelerating Attention Mechanishms in Neural Networks with Approximation</li>
<li>AccPar: Tensor Partitioning for Heterogeneous Deep Learning Accelerator Arrays</li>
<li>PREMA: A Predictive Multi-task Scheduling Algorithm for Preemptible NPUs</li>
<li>Deep Learning Accleration with Neuron-to-Memory Transformation</li>
<li>QuickNN: Memory and Performance Optimization of k-d Tree Based Nearest Neighbor Search for 3D Point Clouds</li>
<li>ALRESCHA: A Lightweight Reconfigurable Sparse-Computation Accelerator</li>
<li>SpArch: Efficient Architecture for Sparse Matrix Multiplication</li>
<li>Communication Lower Bound in Convolution Accelerators</li>
<li>Enabling Highly Efficient Capsule Networks Processing Through A PIM-Based Architecture Design</li>
<li>Fulcrum: A Simplified Control and Access Mechanishm Towards Flexible and Practical In-Situ Accelerators*</li>
<li>Tensaurus: A Versatile Accelerator for Mixed Sparse-Dense Tensor Computations</li>
<li>DRAM-less: Hardware Acceleration of Data Processing with New Memory*</li>
<li>Accelerating Dense Linear Algebra by Exploiting Fine-grain Inductive Behavior*</li>
<li>HyGCN: A GCN Accelerator with Hybrid Architecture</li>
<li>ELP2IM: Efficient and Low Power Bitwise Operation Processing in DRAM*</li>
<li>PIXEL: Photonic Neural Network Accelerator</li>
<li>ResiRCA: A Resilient Energy Harvesting ReRAM-based Accelerator for Intelligent Embedded Processors*</li>
</ol> 
<hr>
<strong>ASPLOS 2020</strong>
<ol>
<li>Prague: High-Performance Heterogeneity-Aware Asynchronous Decentralized Training</li>
<li>NeuMMU: Architectural Support for Efficient Address Translations in Neural Processing Units</li>
<li>AvA: Accelerated Virtualization of Accelerators*</li>
<li>Capuchin: Tensor-based GPU Memory Management for Deep Learning</li>
<li>FlexTensor: An Automatic Schedule Exploration and Optimization Framework for Tensor Computation on Heterogeneous System</li>
<li>Interstellar: Using Halide's Scheduling Language to Analyze DNN Accelerators</li>
<li>Coterie: Exploiting Frame Similarity to Enable High-Quality Multiplayer VR on Commodity Mobile Devices*</li>
<li>BYOC: A "Bring Your Own Core" Framework for Heterogeneous-ISA Research*</li>
<li>HEAX: An Architecture for Computing on Encrypted Data*</li>
<li>Learning Based Memory Allocation for C++ Server Workloads*</li>
<li>Lynx: A SmartNIC-driven Accelerator-Centric Architecture for Network Servers*</li>
<li>SwapAdvisor: Pushing Deep Learning Beyond the GPU Memory Limit via SmartSwapping</li>
<li>DNNGuard: An Elastic Heterogeneous Architecture for DNN Accelerator against Adversarial Attacks</li>
<li>PatDNN: Achieving Real-Time DNN Execution on Mobile Devices with Patter-based Weight Pruning</li>
</ol>
<hr>
<strong>ISCA 2020</strong>
<ol>
<li>High Performance Deep Learning Co-Processor Integrated into x86 SoC with Server-Class CPUs</li>
<li>Data Compression Accelerator on IBM Power9 and Z15 Processors* </li>
<li>Evolution of Samsung Exynos CPU Microarchitecture* </li>
<li>Focused Value Prediction* </li>
<li>DSAGEN: Synthesizing Programmable Spatial Architectures </li>
<li>Gorgon: Accelerating Machine Learning from Relational Data </li>
<li>A Specialized Architecture for Object Serialization with Applications to Big Data Analytics* </li>
<li>NEBULA: A Neuromorphic Spin Based Ultra-Low Power Architecture for SNNs and ANNs </li>
<li>uGEMM: Unary Computing Architecture for GEMM Applications*</li>
<li>Hardware-Software Co-Design for Brain Computer Interfaces*</li>
<li>SysScale: Utilizing Holistic Multi-Domain DVFS to Improve the Energy Efficiency of Mobile Processors*</li>
<li>MLPerfInference: A Benchmarking Methodology for Machine Learning Inference Systems</li>
<li>Mocktails: Capturing the Memory Behavior of Propreitary Mobile Architectures*</li>
<li>Buddy Compression: Enabling Larger Memory for Deep Learning and HPC Workloads on GPUs </li>
<li>Commutative Data Reordering: A New Technique to Reduce Data Movement Energy on Sparse Inference Workloads</li>
<li>RecNMP: Accelerating Personalized Recommendation with Near-Memory Processing </li>
<li>iPIM:Programmable In-Memory Image Processing Accelerator Using Near-Bank Architecture </li>
<li>A Simultaneous Multi-Neural Network Execution Processor Architecture </li>
<li>SmartExchange: A Chiplet-Based Hybrid Sparse-Dense Accelerator for Personalized Recommendations </li>
<li>DeepRecSys: A System for Optimizing End-to-End At-Scale Neural Recommendation Inference </li>
<li>An In-Network Architecture for Accelerating Shared-Memory Multiprocessor Collectives</li>
<li>DRQ: Dynamic Region-Based Quantization for Deep Neural Network Acceleration</li>
<li>Think Fast: A Tensor Streaming Processor (TSP) for Accelerating Deep Learning Workloads</li>
<li>Heat to Power: Thermal Energy Harvesting and Recycling for Warm Water-Cooled Datacenters*</li>
<li>Echo: Compiler-Based GPU Memory Footprint Reduction for LSTM RNN Training </li>
<li>JPEG-ACT: A Frequency Domain Lossy DMA Engine for Training Convolutional Neural Networks</li>
</ol>
<hr>

<strong>*non ML related) </strong> <br>

</body>
</html>



