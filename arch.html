<body style="font-family: monaco">
    <h1>Papers list(* -- non-ML papers) </h1> <br>
    <strong>HPCA 2019</strong>
    <ol>
    <li>HyPar: Torwards Hybrid Parallelism for Deep Learning Accelerator Array</li>
    <li>E-RNN: Design Optimization for Efficient Recurrent Neural Networks in FPGAs</li>
    <li>Bit Prudent In-Cache Acceleration of Deep Convolutional Neural Networks</li>
    <li>Shortcut Mining: Exploiting Cross-Layer Shortcut Reuse in DCNN Accelerators</li>
    <li>Reliability Evaluation of Mixed-Precision Architectures</li>
    <li>Architecting Waferscale Processors - A GPU Case Study</li>
    <li>Machine Learning at Facebook: Understanding Inference at the Edge</li>
    <li>VIP: A Versatile Inference Processor</li>
    <li>String Figure: A Scalable and Elastic Memory Network Architecture</li>
    <li>NAND-Net: Minimizing Computational Complexity of In-Memory Processing of Binary Neural Network</li>
    </ol>
    <hr>
    <strong>ASPLOS 2019</strong>
    <ol>
    <li>Intelligence Beyond the Edge: Inference on Intermittent Embedded Systems</li>
    <li>Boosted Race Trees for Low Energy Classification</li>
    <li>FA3C: FPGA-Accelerated Deep Reinforcement Learning</li>
    <li>PUMA: A Programmable Ultra-Efficient Memristor-based Accelerator for Machine Learning Inference</li>
    <li>FPSA: A Full System Stack Solution for Reconfigurable ReRAM-based NN Accelerator Architecture</li>
    <li>Bit-Tactical A Software/Hardware Approach to Exploiting Value and Bit Sparsity in Neural Networks</li>
    <li>TANGRAM: Optimized Coarse-Grained Dataflow for Scalable NN Accelerators</li>
    <li>Packing Sparse CNNs for Efficient Systolic Array Implementations: Column Combining Under Joing Optimization</li>
    <li>Split-CNN: Splitting Window-based Operations in CNNs for Memory System Optimization</li>
    <li>HoP: Heterogeneity-Aware Decentralized Learning</li>
    <li>Astra: Exploiting Predictability to Optimize Deep Learning</li>
    <li>ADMM-NN: An Algorithm-Hardware Co-Design Framework of DNNs Using Alternating Direction Methods of Multipliers</li>
    </ol>
    <hr>
    <strong>ISCA 2019</strong>
    <ol>
    <li>3D-based Video Understanding Accelerator by Leveraging Temporal Locality and Activation Sparsity</li>
    <li>Sparse ReRAM Engine: Joint Exploration of Activatoin and Weight Sparsity on Compressed Neural Network</li>
    <li>MnnFast: A Fast and Scalable System Architecture for Memory-Augmented Neural Networks</li>
    <li>TIE: Energy-Efficient Tensor Train-Based Inference Engine for Deep Neural Network</li>
    <li>Accelerating Distributed Reinforcement Learning with In-Switch Computing</li>
    <li>Eager Pruning: Algorithm and Architecture Support for Fast Training of Deep Neural Networks</li>
    <li>Laconic Deep Learning Inference Acceleration</li>
    <li>Fractal Machine Learning Computers</li>
    <li>FloatPIM: In-Memory Accelerator of Deep Neural Network Training with High Precision</li>
    </ol>
    <hr>
    <strong>MICRO 2019</strong>
    <ol>
    <li>Wire-Aware Architecture and Dataflow for CNN Accelerators</li>
    <li>Simba: Scaling Deep Learning Inference with Multi-Chip-Module based Accelerators</li>
    <li>ShapeShifter: Enabling Fine-Grain data Width Adaption in Deep Learning</li>
    <li>Connecting RRAMs to Extend Analog Dataflow in an End-to-End In-Memory Processing Paradigm</li>
    <li>ZCOMP: Reducing DNN Cross-Layer Memory Footprint Using Vector Extensions</li>
    <li>Boosting the Performance of CNN Accelerators with Dynamic Fine Grained Channel Gating</li>
    <li>SparTen: A Sparse Tensor Accelerator for Convolutional Neural Networks</li>
    <li>EDEN: Enabling Energy-Efficient High-Performance Deep Neural Network Inference Using Approximate DRAM</li>
    <li>eCNN: A Block-Based and Highly-Parallel CNN Accelerator for Edge Inference</li>
    <li>FlexLearn: Fast and Highly Efficient Brain Simulations Using Flexible On-Chip Learning</li>
    <li>ExTensor: An Accelerator for Sparse Tensor Algebra</li>
    <li>Efficient SpMV Operation for Large and Highly Sparse Matrices Using Scalable Multi-way Merge Parallelization</li>
    <li>Sparse Tensor Core: Algorithm and Hardware Co-Design for Vector-wise Sparse Neural Networks on Modern GPUs</li>
    <li>SMASH: Co-designing Software Compression and Hardware-Accelerated Indexing for Efficient Sparse Matrix Operations</li>
    <li>Tigris: Architecture and Algorithms for 3D Perception in Point Couds</li>
    <li>ASV: Accelerated Stereo Vision System</li>
    <li>Distilling the Essence of Raw Video to Reduce Memory Usage and Energy at Edge Devices</li>
    <li>MANIC: A Vector-Dataflow Architecture for Ultra-Low-Power Embedded Systems</li>
    <li>NetDIMM: Low-Latency Near-Memory Network Interface Architecture</li>
    <li>TensorDIMM: A Practical Near-Memory Processing Architecture for Embeddings and Tensor Operations in Deep Learning </li>
    <li>Understanding Reuse, Performance, and Hardware Cost of DNN Dataflow: A Data-Centric Approach</li>
    <li>MaxNVM: Maximizing DNN Storage Density and Inference Efficiency with Sparse Encoding and Error Mitigation</li>
    <li>Neuron-Level Fuzzy Memorization in RNNs</li>
    <li>Manna: An Accelerator for Memory-Augmented Neural Networks</li>
    </ol>
    <hr>
    <strong>ASPLOS 2020</strong>
    <ol>
    <li>Prague: High-Performance Heterogeneity-Aware Asynchronous Decentralized Training</li>
    <li>NeuMMU: Architectural Support for Efficient Address Translations in Neural Processing Units</li>
    <li>AvA: Accelerated Virtualization of Accelerators*</li>
    <li>Capuchin: Tensor-based GPU Memory Management for Deep Learning</li>
    <li>FlexTensor: An Automatic Schedule Exploration and Optimization Framework for Tensor Computation on Heterogeneous System</li>
    <li>Interstellar: Using Halide's Scheduling Language to Analyze DNN Accelerators</li>
    <li>Coterie: Exploiting Frame Similarity to Enable High-Quality Multiplayer VR on Commodity Mobile Devices*</li>
    <li>BYOC: A "Bring Your Own Core" Framework for Heterogeneous-ISA Research*</li>
    <li>HEAX: An Architecture for Computing on Encrypted Data*</li>
    <li>Learning Based Memory Allocation for C++ Server Workloads*</li>
    <li>Lynx: A SmartNIC-driven Accelerator-Centric Architecture for Network Servers*</li>
    <li>SwapAdvisor: Pushing Deep Learning Beyond the GPU Memory Limit via SmartSwapping</li>
    <li>DNNGuard: An Elastic Heterogeneous Architecture for DNN Accelerator against Adversarial Attacks</li>
    <li>PatDNN: Achieving Real-Time DNN Execution on Mobile Devices with Patter-based Weight Pruning</li>
    </ol>
    <hr>


</body>
