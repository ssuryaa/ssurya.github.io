<body style="font-family: monaco">

    <strong>ICLR 2019</strong>
    <ol>
    <li>ProxylessNAS: Direct Neural Architecture Search on Target Task and Hardware</li>
    <li>Defensive Quantization: When Efficiency Meets Robustness</li>
    <li>Accumulation Bit-width Scaling for Ultra-low Precision Training of Deep Networks</li>
    <li>Energy-constrained Compression for Deep Neural Networks via Weighted Sprase Projection and Layer Input Masking</li>
    <li>Understanding Straight-Through Estimator in Training Activation Quantized Neural Nets </li>
    <li>Per-Tensor Fixed-Point Quantization of the Back-Propagation Algorithm</li>
    <li>Local SGD Converges Faster and Commmunicates Little</li>
    </ol>
    <hr>
    <strong>CVPR 2019</strong>
    <ol>
    <li>HAQ: Hardware-Aware Automated Quantization with Mixed Precision</li>
    </ol>
    <hr>
    <strong>NeurIPS 2019</strong>
    <ol>
    <li>Deep Leakage from Gradients</li>
    <li>Post Training 4-bit Quantization of Convolutional Networks for Rapid Deployment</li>
    <li>Dimension Free Bounds for Low-Precision Training</li>
    <li>AutoAssist: A Framework to Accelerate Training of Deep Neural Networks</li>
    <li>Backprop with Approximate Activations for Memory-efficient Network Training</li>
    <li>E<sup>2</sup>-Train: Training State-of-the-art CNNs with Over 80% Energy Savings</li>
    <li>Hybrid 8-bit Floating Point (H8FP) for Training and Inference for Deep Neural Networks</li>
    <li>Qsparse-local-SGD: Distributed SGD with Quantization, Sparsification, and Local Computations</li>
    <li>Communication-Efficient Distributed Learning via Lazily Aggregated Quantized Gradients</li>
    <li>Double Quantization for Communication-Efficient Distributed Optimization</li>
    <li>Focused Quantization of Sparse CNNs</li>
    </ol>
    <hr>
    <strong>ICML 2019</strong>
    <ol>
    <li>Parameter Efficient Training of Deep CNNs by Dynamic Sparse Reparameterization</li>
    <li>Stochastic Weigt Averaging in Low Precision Training</li>
    <li>Improving Neural Network Quantization without Retraining Using Outlier Channel Splitting</li>
    </ol>
    <hr>
    <strong>ICML 2018</strong>
    <ol>
    <li>Error Compensated Quantized SGD and its Application to Large Scale Distributed Optimization</li>
    </ol>

    
</body>
