<body style="font-family: Calibri">

    <strong>ICLR 2019</strong>
    <ol>
    <li>ProxylessNAS: Direct Neural Architecture Search on Target Task and Hardware</li>
    <li>Defensive Quantization: When Efficiency Meets Robustness</li>
    <li>Accumulation Bit-width Scaling for Ultra-low Precision Training of Deep Networks</li>
    <li>Energy-constrained Compression for Deep Neural Networks via Weighted Sprase Projection and Layer Input Masking</li>
    <li>Understanding Straight-Through Estimator in Training Activation Quantized Neural Nets </li>
    <li>Per-Tensor Fixed-Point Quantization of the Back-Propagation Algorithm</li>
    <li>Local SGD Converges Faster and Commmunicates Little</li>
    </ol>
    <hr>
    <strong>CVPR 2019</strong>
    <ol>
    <li>HAQ: Hardware-Aware Automated Quantization with Mixed Precision</li>
    </ol>
    <hr>
    <strong>NeurIPS 2019</strong>
    <ol>
    <li>Deep Leakage from Gradients</li>
    <li>Post Training 4-bit Quantization of Convolutional Networks for Rapid Deployment</li>
    <li>Dimension Free Bounds for Low-Precision Training</li>
    <li>AutoAssist: A Framework to Accelerate Training of Deep Neural Networks</li>
    <li>Backprop with Approximate Activations for Memory-efficient Network Training</li>
    <li>E<sup>2</sup>-Train: Training State-of-the-art CNNs with Over 80% Energy Savings</li>
    <li>Hybrid 8-bit Floating Point (H8FP) for Training and Inference for Deep Neural Networks</li>
    <li>Qsparse-local-SGD: Distributed SGD with Quantization, Sparsification, and Local Computations</li>
    <li>Communication-Efficient Distributed Learning via Lazily Aggregated Quantized Gradients</li>
    <li>Double Quantization for Communication-Efficient Distributed Optimization</li>
    <li>Focused Quantization of Sparse CNNs</li>
    </ol>
    <hr>
    <strong>ICML 2019</strong>
    <ol>
    <li>Parameter Efficient Training of Deep CNNs by Dynamic Sparse Reparameterization</li>
    <li>Stochastic Weigt Averaging in Low Precision Training</li>
    <li>Improving Neural Network Quantization without Retraining Using Outlier Channel Splitting</li>
    </ol>
    <hr>
    <strong>ICML 2018</strong>
    <ol>
    <li>Error Compensated Quantized SGD and its Application to Large Scale Distributed Optimization</li>
    </ol>
    <strong>ICLR 2020</strong>
    <ol>
    <li>Precision Gating: Improving Neural Network Efficiency with Dynamic Dual-Precision Activations</li>
    <li>Mixed Precision DNNs: All You Need is a Good Parameterization</li>
    <li>Neural Epitome Search for Architecture-Agnostic Network Compression</li>
    <li>Training Binary Neural Networks with Real-to-Binary Convolutions</li>
    <li>Budgeted Training: Rethinking Deep Neural Network Training Under Resource Constraints</li>
    <li>AtomNAS: Fine-Grained End-to-End Neural Architecture Search</li>
    <li>And the Bit Goes Down: Revisiting the Quantization of Neural Networks</li>
    <li>Don't Use Large Mini-Batches, use Local SGD</li>
    <li>Fair Resource Allocation in Federated Learning</li>
    <li>Distributed Backdoor Attacks Against Federated Learning</li>
    </ol>

    
</body>
